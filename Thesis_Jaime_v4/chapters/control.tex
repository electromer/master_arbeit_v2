\chapter{Minimization on Torque Level for an 8-DOF Robot}
\label{ch:control}





\section{Projected Gradient Minimization}
\label{sec:Gradientbasedminimization}


 %
% 
% XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%  XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%   XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%    XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
% 
%\textbf{ THE FOLLOWING APPLIES SO FAR ONLY TO VELOCITIES:} \\
%\TODO{USING NEMEC-MINE version...If the scaling gain is high it  oscillates around a point that is not a minimum. But with small gains it optimizes fine though...DOES THE CONDITION OF SPDness DO NOT APPLY TO THE PROJECTOR IN THE TORQUES??? In the papers I found it always refers to joint level :/ } 
%\correction{in matlab postmultiplying by $M$ or premultiplying by $M^{-1}$ return P.D. matrices numerically tested} \\
%\correction{if I cannot prove just say that the prerotation always applies for the velocity. All the papers where they are used (only in velocity level) refer to the Foundations of robotics of Yoshikawa to which I do not have acces.}
%%
%\textcolor{magenta}{ This null space may be used to optimize some specific function $\mathrm{{p}={p}(\mathbf{q})}$. Now let us define its gradient $ \mathrm{\mathbf{\phi} =  \nabla {p}(\mathbf{q})}$ and a scaling factor  $k$. According to \cite{yoshikawa}, $\mathrm{p}$ is minimized for any scalar $k < 0$, and maximized for any scalar $k > 0$. This holds	as long as $\mathrm{\phi^{T} \mathbf{N} \phi}$ is semidefinite positive, being $\mathrm{\mathbf{N}}$ the null space projector, either in torque or velocity level. For the Moor-Penrose pseudoinverse this is true since the the null space projector is positive definite. For the mass weighted pseudoinverse the projector is non-definite since it is not symmetrical. Using the mass weighted pseudoinverse, the null space projector in for the joint velocities ($\mathrm{\mathbf{N_{\dot{q}}}}$) may be forced to be positive definite, by postmuliplying it by $\mathrm{\mathbf{M^{-1}}}$ as proposed in \cite{Nemec}. This ensures the best optimization step for the mass weighted pseudoinverse.} \\
% 
% 
%
%
%
%In torque level a different matrix must be taken to force positive definiteness. Let us take the mass matrix  $\mathrm{\mathbf{M}}$ instead.
%
%
%
%
%\begin{lemma}
%	Given the null space projector   $\mathrm{\mathbf{N_{\tau}}} = \mathrm{(\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T})}$, where $\mathrm{\mathbf{J}^{\#}}$ is the mass weighted pseudoinverse \cite{khatib1995}. Then the form
%	\begin{equation}
%	\phi^{T} \mathbf{N_{\tau}} \mathbf{M} \phi
%	\label{eq:nemec_torque}
%	\end{equation}  is positive semi-definite.
%\end{lemma}
%
%\begin{proof}
%	First the symmetry is to be proven.
%	\begin{equation}
%	\begin{aligned}
%	\phi^{T} (\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T}) \mathbf{M} \phi = \\
%	\phi^{T} (\mathbf{M} - \mathbf{J}^T 
%	\mathbf{ (J M^{-1} J^{T})^{-1}	J  M^{-1}	}\mathbf{M} ) \phi .
%	\end{aligned}	
%	\end{equation}
%	
%	The term $\mathrm{(\mathbf{M} - \mathbf{J}^T \mathbf{ (J M^{-1} J^{T})^{-1}	J })}$ is always symmetrical since $\mathrm{\mathbf{M}}$ is symmetrical.
%\end{proof}
%
%\begin{proof}
%	Now we have to prove that (\ref{eq:nemec_torque}) is positive semi-definite. For that the symmetric mass matrix is decomposed $\mathrm{\mathbf{M = U^{T} U}}$.
%	%
%	\begin{equation}
%	\begin{aligned}
%	\phi^{T} (\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T}) \mathbf{U^{T} U} \phi = \\
%	\phi^{T} (\mathbf{U^{T} U} - \mathbf{J}^T 
%	\mathbf{ (J (U^{T} U)^{-1} J^{T})^{-1}	J } )\phi = \\
%	\phi^{T} (\mathbf{U^{T} U} - \mathbf{J}^T 
%	\mathbf{ (J (U^{T} U)^{-1} J^{T})^{-1}	J } \mathbf{(U^{T} U)}^{-1}	\mathbf{(U^{T} U)}) \phi 
%	\end{aligned}
%	\end{equation}
%	
%	Substituting $z=J U^{T}$ and $z=J U^{T}$
%	
%	
%\end{proof}
%	
%
%
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%
The first approach to consider is the projection of the gradient into the null space using the null space projector $\mathrm{\mathbf{N_{\tau}}} = \mathrm{(\mathbf{I} - \mathbf{J}^T \mathbf{J}^{\#T})}$. This projector is used in order to ensure static and dynamic consistency \cite{khatib1995}.  The  projector will minimize the reflected mass $\mathrm{m_u}$ inside the null space of the first task, as long as $\mathrm{k}$ is a positive scalar (see (\ref{eq:gbm_nico})).  




 As with the 7-DOF LWR, the 8-DOF robot shows bad performance close to local maxima. The rotational joints are not able to overcome the friction even for high values of $\mathrm{k}$. This problem is not present in the linear axis which has a position controller. The linear axis is included into the whole-body impedance control framework using an admittance interface \cite{whole_body_imp}. Therefore, even small values for the torque cause a displacement of this joint.
  However, this movement is oscillatory, specially for high gains. These oscillations appear around the starting position of the linear axis.
  



%% HERE THE SADDLE POINTS DO NOT MAKE SENSE ANYMORE. And not only close to maxima, but close to saddle point, which makes sense as the null  problem is characteristic of two dimensional data. 



%One could think of obtaining new equations of motion for the robot, where the constraint of null space movement would be satisfied. Which using the Lagragian approach would be getting from 
%
%\begin{equation}
%M(\mathbf{q})\ddot{\mathbf{q}}+C(\mathbf{q},\dot{\mathbf{q}})+G(\mathbf{q})=\mathbf{\tau} +J_c*\mathbf{\Lambda},
%\label{joint_space_dynamics}
%\end{equation}	
%to	
%
%\begin{equation}
%\bar{M}(\mathbf{q})\ddot{\mathbf{q}}+\bar{C}(\mathbf{q},\dot{\mathbf{q}})+\bar{G}(\mathbf{q})=\bar{\mathbf{\tau}},
%\label{new_joint_space_dynamics}
%\end{equation}	
%
%where $J_c$ is the [6x8] constrained Jacobian that express the null space movement and $\Lambda$ are the lagrangian multipliers. But one can see that none of the matrices in the new equation of motion would depend on $q_1$ as none of the originals depends on $q_1$, so this would actually solve nothing.














\section{Minimization Based on Attractive Potential}
\label{sec:Minimattractivepotential}




For the potential field  defined in (\ref{eq:potential_intro}) the aim is to find a suitable goal position, where the reflected mass on the tip of the robot in certain direction is minimal. Two possible goal positions are  the global minimum and a close local minimum. Global minimization may be done determining first all possible null space positions. And second,  finding the local minima and global minimum. This is treated in  Section \ref{sec:globaloptimization}. For the local minimization an iterative solution is discussed in  Section \ref{sec:localoptimization}.
Like it occurred with the 7-DOF LWR, since the goal position is chosen to be far from local maxima the problem from the previous approach will not appear here.

%\begin{equation}
%U(\mathbf{q}) = - \frac{1}{2} Kp (\mathbf{q}_{m_u}^\ast - \mathbf{q})^T (\mathbf{q}_{m_u}^\ast - \mathbf{q}). \label{eq:potential_extension8}
%\end{equation}

%Differentiating we obtain a torque to project into the null space
%
%\begin{equation}
%\mathbf{\tau}_{m_u}^\ast = \frac{\partial {U(\mathbf{q})}}{\partial {\mathbf{q}}} = - k (\mathbf{q}_{m_u}^\ast - \mathbf{q}).
%\end{equation}
%
%
%\begin{equation}
%\mathbf{\tau}_d = \mathbf{\tau}_\mathrm{imp} + (I - J^T J^{\#T}) \mathbf{\tau}_{m_u}^\ast, \label{eq:potential_controller_extension8}
%\end{equation}





%\section{Conclusion}
%\label{sec:conclusion_ext8}



\subsection{Global Optimization}
\label{ch:globaloptimization}
\label{sec:globaloptimization}


From \cite{fabianthesis} we already have the null space positions as a point cloud. This numerically obtained dataset is used as input in Section \ref{sec:global_numerical} for an algorithm that returns the local and global extrema of the grid. However, an analytical form of the grid is of interest. Therefore, first the Section \ref{sec:global_analytical} is dedicated to obtain an analytical form  of this null space grid.




\subsubsection{Analytically}
\label{sec:global_analytical}



%\subsection{Imposing the null space constraint in the reflected mass}
\label{subsec:imposing_ns_constraint}


The goal is to reformulate $m_u$, so it does include the null space constraint. 
For sake of simplicity the problem is first treated over the 7-DOF LWR.  We aim to get $\mathrm{m_u(\mathbf{q}_{ns})}$ where $\mathrm{\mathbf{q}_{ns}}$ is


\begin{equation}
\mathbf{q}_{ns}(t) = \int_{t_0}^t \! \ker(\mathbf{J}(\mathbf{q}(\tau))) \, \mathrm{d}\tau. 
\label{eq:integral_kernel}
\end{equation}

For the 7-DOF LWR the analytical formula of the kernel of the Jacobian is:

\begin{equation}
\ker(\mathbf{J}(\mathbf{q}))  =\begin{pmatrix} -0.06084s_6 c_3 s_4^2\\
0. 6084s_6 s_4^2 s_3 s_2\\
0.00156s_4 s_6 (-40s_2)...\\
...-39s_2 c_4 +39c_2 c_3 s_4\\
0\\
-0.00156s_4 s_2 (-40c_4 s_6)+...\\
...40s_4 c_5 c_6 -39s_6\\
-0.06241s_6 s_4^2 s_2 s_5\\
0.0624s_4)^2 s_2 c_5	
\end{pmatrix}	,
\label{eq:dq_ns}
\end{equation}


where $\mathrm{s_i}$ and $\mathrm{c_i}$ are respectively the sinus and cosine of the $\mathrm{q_i}$ joint. For this kernel of the Jacobian, the integral of the equation (\ref{eq:integral_kernel}) is analytically not solvable with software like Maple or Mathematica. 
Another way may be obtaining the anti-derivatives of the entries of (\ref{eq:dq_ns}) separately. If one can prove that one of this entries has no closed-form, it would mean that the kernel has no antiderivative. Let us take the last term of the fifth entry, $\mathrm{s_6}$. From the numerical integration done in  \cite{fabianthesis} it is known that the sixth rotational joint takes the form of a sinus. Therefore, this term can be rewriten as $\mathrm{\mathrm{sin(sin(x))}}$.  
Mathematica is not able to return this integral as a composition of elementary functions because they are not built into the Wolfram Language \cite{wolfram}, and Mathematica can rarely change variables automatically when integrating. This function may however be represented as a suitably generalized Kampé de Fériet hypergeometric function \cite{kampe} of two variables. Although this integral has a closed form, most of the remaining integrals, which cannot be solved via software, have most likely no closed form. Like for example $\mathrm{sin(sin(ax))sin(sin(bx))sin(cx)}$, for any value of $\mathrm{a,b,c}$, which comes from the simplification of the velocity of the last rotational joint. However, proving that all the entries in (\ref{eq:dq_ns}) have no antiderivative, it is not straightforward and it is left to future work. It is worth mentioning that classic approaches to find the integral of the kernel of the Jacobian do not work. For example, the Risch algorithm \cite{risch}, which attempts to prove Liouville's theorem \cite{lioville} for the existence of antiderivatives as rational functions, does not work because it is already embedded into Mathematica and this software is not able to solve these integrals.
 
%\textcolor{red}{ARE THE OTHER FUNCTIONS ALSO INTEGRABLE??}
%%
%
%\textbf{THE REST I GUESS IS WORTHLESS BECAUSE THE ANTIDERIVATIVE IT ACTUALLY EXISTS:}\\
%This function is Riemann integrable, but this does not mean that a closed-form of the integral exists. \\
%Unfortunately $\mathrm{sin(sin(x))}$ has most likely no antiderivative. It can only be evaluated in specific intervals, for which tables, like the ones in \cite{hanbook_math}, provide a result depending on Struve and Bessel functions \cite{struve}. In the intervals where the integral cannot be evaluated, it may be representable as "incomplete" Struve and Bessel functions, which nowadays may only be approximately computed using special algorithms \cite{incomplete_bessel}. One may argue if these solutions for the definite integral are closed-form or not. In general a solution is said to be "closed-form" if it can be expressed analytically in terms of some "well-known" functions \cite{def_closed_form},  and nowadays not all the authors accept "incomplete" Struve functions as closed-form.  \\
%Software like Mathematica compute elementary antiderivatives using approaches like the Risch algorithm \cite{risch}, which attempts to prove Liouville's theorem \cite{lioville} for the existence of antiderivatives as rational functions.  An explicit proof of the non-existence of this antiderivative  by hand is left to future work.
%
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\
%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXx \\





For our case with the 8-DOF robot, an attempt is done using curve fitting. If successful, this would generate a function, from this point cloud, that could be used as an input to the solvers provided by MATLAB Optimization Toolbox. For this the function \textit{fit} from MATLAB is used. This functions allows to fit a curve to a surface up to a degree of five for each of the inputs ($\mathrm{q_1}$ and $\mathrm{q_4}$ in our case).  However, the obtained functions do not resemble enough the original grid because the degree of $\mathrm{q_1}$ and $\mathrm{q_4}$ needs to be higher than five. The difficulty increases when the robot is following a trajectory because the surface changes with the direction of the end-effector. One possible approach could be the use of neural networks to find a better fitting of the curve, but this is out of the scope of this thesis. Being this approach discarded, one needs to work with numerical values. 





\subsubsection{Numerically}
\label{sec:global_numerical}


In general the minimization of the reflected mass must be performed while the robot is following a trajectory. The algorithm  implemented on this thesis shows to be significantly faster than the standard algorithms to obtain the local extrema \cite{Lemire}. However, the computation of the grid with enough accuracy requires at least 2 s.  This precludes from online capabilities.  On the other hand, knowing the global minimum may be still interesting if the trajectory is previously known. In this way, the computation of these grids and the search of the global minima may be offline.\\
%
Another problem that may be solved offline is the computation of the path to the minimum. This is necessary because the robot may go through local peaks of maximum reflected mass, on his trajectory to the global minimum. This may be solved with path optimization techniques, out of the scope of this work. 

%
One way to see the numerical values of the grid is as image-points of a picture. Where the third coordinate of each point represents a property of the image at that point, like for example the color. From this point of view the dataset, that is the point cloud, may be treated with tools from the MATLAB Image Processing Toolbox. These can be used to write an algorithm that finds the minima in the data set. Not having a license for such a Toolbox another implementation is necessary.
Without making use of this Toolbox a filter \cite{minmaxfilter} to find the local maxima and minima from a given dataset is used. This filter computes the maxima and minima over running windows of a determined size. The concept “\textit{window}” here refers to the image-processing domain. Given a pixel value at a point \textit{(x,y)} the window limits an area (usually squared) around the pixel, where the extreme local values are searched. The \textit{pixel} is in our case  the value of the reflected mass.\\
%
In the filter, the sliding window can be a scalar (in which case the same window size will be scanned for all dimensions) or an array (in which case separate scan sizes will be run for each dimension). The advantage of this filter is that it can do multidimensional filtering. Here the signal is two dimensional, i.e. with two independent variables, namely the linear axis ($\mathrm{q_1}$) and the fourth joint ($\mathrm{q_4}$). Therefore the window-size should have two entries (or one if one wants unidimensional filtering).


Here, two examples are explained for better understanding:
\begin{enumerate}
	\item In the first case (Fig. \ref{fig:maxminfilter1}) the window size is set to $[dim_{max_{1}}, \ 1]$. The local minimum is found in each self-motion manifold slice, for constant positions of the fourth joint.
	\item In the second case (Fig. \ref{fig:maxminfilter2}) the window size is set to $[1, \ dim_{max_{2}}]$. The local minimum is found in each self-motion manifold slice, for constant positions of the linear axis.
	
\end{enumerate}

Where $dim_{max_{i}}$ is the data length of the $ith$ dimension. It is clear to see that when setting both dimensions to their relative maximum values, $[dim_{max_{1}}, \ dim_{max_{2}}]$, only the global extrema are found.


\begin{figure}[htb]
	\centerline{
		\includegraphics[width=0.7\textwidth]{images/global_q4_cte_v2_new.eps}}
	\caption{Plot of the reflected mass in x direction against the self-motion
		manifold coordinates. Local minima found in each self-motion manifold slice for each fourth joint angle. }
	\label{fig:maxminfilter1}
\end{figure}

\begin{figure}[htb]
	\centerline{
 		\includegraphics[width=0.7\textwidth]{images/global_q1_cte_v2_new.eps}}
	\caption{ Plot of the reflected mass in x direction against the self-motion
		manifold coordinates. Local minima found in each self-motion manifold slice for each linear axis position. }
	\label{fig:maxminfilter2}
\end{figure}





\paragraph{The Algorithm}
\label{subsec:global_alg}

The filter is based on a \textit{Monotonic Wedge} algorithm \cite{Lemire}.
A simpler algorithm is the \textit{Max Queue} algorithm. In this last one, there is a queue with a fixed capacity and values are pushed into it. When a value is pushed another one is pulled. The values are continuously pushed while the maximum from the queue is written in a results-row.
It uses the separability of the problem.  For example, in a two dimensional grid, it goes through all the rows and then through all the columns, making it two separate one dimensional problems.\\
%
Although intuitive and easy to implement it does not work well with pseudo-monotonic data. And in our case, the null space motion grids of the reflected mass may be divided into pseudo monotonic areas. The \textit{Monotonic Wedge} algorithm performs better in these cases, therefore this algorithm is preferred here. This algorithm also ensures that new maximums are quickly found when moving out of the window. However, as we are optimizing offline this is not a defining advantage.
%Although the speed is not a relevant factor as we are treating the data offline.

%Like before the values of the reflected mass are being pushed and pulled. But instead of searching the elements through the whole window the algorithm just takes the next element from the wedge.

%\underline{Short introduction to wedges:} Let \textbf{$U_i$}  be a monotonic wedge of the sequence \textbf{$A_i$} . Then \textbf{$U_i$}  has the following properties:

%\textbf{$U_0$} is the index of the maximum element of \textbf{$A_{U_0}$} ,

%U1  is the largest element of all elements to the right of \textbf{$A_{U_0}$} .
%In general, \textbf{$U_k$}  is the largest element of all elements to the right of \textbf{$A_{U_{k-1}}$}.
%{\color{red} add more explanation and references about this?? if needed see more explanation and other 2D approaches in the pdf minmaxfilters.pdf. IT WOULD BE INTERESTING COMPARING THE OTHER APPROACHES AND SAYING WHY I DECIDED FOR THIS ONE!!!!}

%The advantage of this algorithm is mainly the speed {\color{red} FIND A BETTER ADVANTAGE because if this approach is only for offline optimization actually it does not important that much the speed!! }  because it requires, in the worst case, 3 comparisons per element.

The standard way to find the minima of a given grid is with small sliding windows. A more accurate way is: First finding all the minima in each self-motion manifold slice with constant position of the fourth joint (see Fig. \ref{fig:maxminfilter1}). Secondly doing an equivalent minimization for each position of the linear axis (see Fig. \ref{fig:maxminfilter2}). 
And finally getting the common minima from both sets. This strategy was implemented here, and performs better than simply filtering with small sliding windows. 

\paragraph{Continuity of the Global Minimum}

A requirement for the goal position is its continuity. Following a trajectory the different joint configurations lead to different grids. However, as long as the direction does not change abruptly (and this is the case considered in this work), the minimum shows continuity. In the sense that every computed global minimum does not lay far from the previous one. An exception occurs when the grid shows contour lines. If contour lines exist, where local minima lay. And the global minimum is in this contour line. Due to small computational errors when computing the grid, the global minimum suffer of discontinuity. In the tests run, when the robot is performing a null space motion, the global minimum is computed in different points of the contour line. Meaning that the global minimum is prone to discontinuities even with a static end-effector. This makes the global minimum an undesired goal position.
It is also worth mentioning that in the y- and z- direction the grid shows contour lines where the global minimum lays. Meaning that in these directions local minimization provides as good results, in terms of optimization, as the global minimization. 




















\subsection{Local Optimization}
\label{ch:localoptimization}
\label{sec:localoptimization}


This section considers the search of the 
local minimum in the reflected mass, for setting it as goal position in the attractive potential from (\ref{eq:potential_intro}). Classical local minimization techniques can be divided  into Trust Region and Line Search   techniques. In this section, a review on both, for the null space of the 8-DOF robot, is provided.



\subsubsection{Iterative Strategies}
\label{sec:iterative_strategies}


\paragraph{Trust Region}
\label{subsec:tregion}

Given an objective function to minimize. This  strategy firstly aims to obtain a subset of the region of this objective function. The subset has to contain the current point. Secondly it finds a minimum in this subset. The subset is often approximated using a quadratic function. This approximated function has a similar behavior to the original objective function in the vicinity of the current point. If the trusted region is small enough, a step size will be found that decreases the objective function sufficiently. This makes sense since for general nonlinear functions a local approximation (like linear approximation or quadratic approximation)  is only locally valid. \\ 
%
One of the requirements is then an analytical objective function, which will be locally approximated. In this case no analytic solution has been found (see Section \ref{subsec:imposing_ns_constraint}) which  provides all the possible null space positions, i.e. there is no analytical form of the objective function.\\
%
%In Section \ref{sec:global_analytical} an attempt was done to find this analytical form using curve fitting. But first this required the point cloud of the data, so it suited only global minimization. And second this objective function is not independent of the direction of motion or the initial joint configuration.
%
Another approach would be computing only a subset of the grid in a small region enclosing the current point. And find an analytical form which approximates the shape of this region. However, once the points are computed, it would be easier to compare the masses of these points than creating a function that would contain them in order to apply the Trust Region method. Therefore these methods are of no interest for this work.





\paragraph{Line Search}
\label{subsec:lsearch}



In the these techniques, starting from the current iteration, a descent direction is chosen in order to minimize the objective function. Then the step size is computed to determine the distance to move in this direction. In each iteration the direction and the step size need to be computed. The descent direction can be computed by various methods, such as gradient descent (GD), Newton's methods (NM) or Quasi-Newton's methods (QNM). % And the step size can be determined either exactly or inexactly.
Both Line Search methods and Trust Region methods generate a step using a model of the objective function. Therefore the lack of an analytical form as objective function arises here as in the Trust Region strategy.
However, while the Trust Region methods use this model to generate a region, the Line Search methods use it to compute the descent direction.  
This problem of finding the descent direction can be addressed as finding null space velocity that minimizes the reflected mass. This velocity may be selected by a  select-function :
%
\begin{align}
\text{select}: \mathbf{C}\rightarrow\ker(\mathbf{J}(\mathbf{q})),\; \mathbf{q}\mapsto\dot{\mathbf{q}} .
\end{align}
%
 The search of a null space velocity that minimizes the reflected mass is treated in the next chapter. 








