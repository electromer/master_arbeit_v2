\chapter{Minimization on Velocity Level for an 8-DOF Robot}
\label{ch:velocity}

The problem of finding a local minimum requires a null space velocity that minimizes the reflected mass.
Optimization of a secondary task at velocity level is an extensively discussed topic. Some optimization schemes can be found in \cite{opt_vel_level_1} and \cite{opt_vel_level_2}.  The first idea proposed in this work is to project the gradient of the reflected mass into the null space. For which two approaches are considered: Project Gradient (PG) \cite{JLA_1} and the Reduced Gradient method (RG) \cite{PG_RG}. This chapter begins with a review on both methods in Section \ref{sec:PGvsRG}.
 The RG is treated in detail in Section \ref{sec:1D2D}, where the proposed algorithm to find local minima is introduced. This RG is computed analytically and numerically.  
 A theoretical comparison of both approaches is done in this chapter while the practical comparison, after implementation in the robot, is described in  Chapter \ref{ch:experiments}. Making use of the RG technique, decoupling on the minimization is achieved in Section \ref{sec:Decoupling}. This decoupling consists in separate, and weight, the minimization of the reflected mass w.r.t. both redundant joints. The possibility of taking advantage of this decoupling to avoid joint limits is presented in Section \ref{subsec:wf_JLA}.
Besides Gradient Descent (GD),  other Line Search minimization schemes are considered and tested. The chapter concludes with a comparison of all the minimization schemes in  Section \ref{sec:comparison_local_minim}.
 % In the analytical approach, the velocities of the redundant joints($q_1$ and $q_4$) are determined imposing	 a decrease of the mass w.r.t. time. In the numerical approach, the change of the mass w.r.t. the redundant joints is obtained in a numerical way. The velocity of the remaining joints is computed in both approaches imposing the null space constraint.
 
 


\section{Projected vs Reduced Gradient}
\label{sec:PGvsRG}
%\subsubsection{Selection of descent direction}
%\label{subsubsec:desc_direction}

The main difference between both approaches is the dimension of the gradient.  In PG the eight-dimensional gradient is projected into the null space, while in RG the gradient only w.r.t. the redundant joints ($\mathrm{q_1}$ and $\mathrm{q_4}$ in this case) is considered.
PG is analog to the minimization approach discussed in Section \ref{sec:Gradientbasedminimization} at torque level. Using the projector from (\ref{eq:proj_dq_khatib}), the null space velocity is:


\begin{equation}
\mathbf{\dot{q}}_{ns} =  - k (\mathbf{I} -  \mathbf{J}^{\#} \mathbf{J} ) \nabla m_u(\mathbf{q}), \label{eq:RG_dq}
\end{equation}

where  $\mathrm{k}$ is again a positive scaling gain and $\mathrm{\nabla m_u(\mathbf{q})}$ is an eight-dimensional vector.\\
%
%
%\st{ If the velocity is normalized close to the minimum high oscillations appear in all the joints, even for small $k$. If it is not normalized, better behavior is achieved but one must take care of choosing a small step size so the velocities are not too high. Because if they are too high the new position obtained may not lay on the null space.} \textcolor{green}{ HERE THE MINIMIZATION IS ACTUALLY FUCKED BECAUSE THE PROJECTOR IS NOT POSITIVE DEFINITE!!it is necessary to use Nemec's projector!!!} \textcolor{red}{With the normal method (without rotating gradient) the projector is often negative semidefinite so it maximizes when it should minimize, and the other way around... \textbf{THE MINIMIZATION IS NOT ENSURED!!!}(as Nemec says)}
%
%
%
In (\ref{eq:RG_dq}) the Moore-Penrose pseudoinverse is firstly chosen for the projector. 
However, many different projectors can be used. As an alternative to the previous one, a projector based on the mass weighted pseudoinverse, is also considered here. The null space projector has to be nonnegative definite to ensure that, for any $\mathrm{k > 0}$, the mass is minimized. A proof of this can be found in \cite{yoshikawa}. Using the Moore-pseudoinverse, the null space projector is always positive definite, but using the mass weighted pseudoinverse the null space projector is not symmetrical, hence non-definite. Therefore, in order to ensure minimization, it is necessary to post-multiplicate the mass weighted projector by the inverse of the mass matrix, $\mathbf{M}^{-1}$  \cite{Nemec}. The null space velocity is in this case: 

\begin{equation}
	\mathbf{\dot{q}}_{ns} =  - k (\mathbf{I} -  \mathbf{J}^{\#MT} \mathbf{J} ) \mathbf{M}^{-1} \nabla {m}_u(\mathbf{q}). \label{eq:RG_dq_nemec}
\end{equation}






Another approach worth considering is the Reduced Gradient (RG). 
This gradient regards the change of the reflected mass w.r.t. the redundant joint only:

\begin{equation}
\nabla m_u'(\mathbf{q}) = \left[
\frac{\partial {m_u'(\mathbf{q})}}{\partial {q_1}}, \   \frac{\partial {m_u'(\mathbf{q})}}{\partial {q_4}} \right]^{T}. \label{eq:grad_refl_mass_RG}
\end{equation}

This gradient is used for the velocity of the redundant joints, while for the velocity of the non-redundant joints the null space constraint has to be imposed. Given a null space motion, the following is satisfied:
\begin{equation}
\mathbf{J} \dot{\mathbf{q}} = 0 .
\label{eq:ns_motion}
\end{equation}

And separating between redundant (${\mathbf{q}}_{r}$) and non redundant (${\mathbf{q}}_{nr}$) joints we can express the previous equation as:
\begin{equation}
[\mathbf{J}_r, \ \mathbf{J}_{nr}] [\mathbf{\dot{q}}_r, \  \dot{\mathbf{q}}_{nr}]^T = 0    ,
\label{eq:}
\end{equation}

\begin{equation}
\dot{\mathbf{q}}_{nr} =  - \mathbf{J}_{nr}^{-1} \mathbf{J}_r \mathbf{\dot{q}}_r    ,
\label{eq:ns_constraint}
\end{equation}

where $\mathbf{J}_{nr}$ and $\mathbf{J}_{r}$ are the Jacobians associated to the non-redundant and redundant joints respectively. This approach is often used in parallel manipulators where some of the joints are active and other passive. A more detailed discussion can be found in \cite{Murray:1994:MIR:561828}. The null space velocity defined by \cite{reduced_gradient} is

\begin{equation}
\mathbf{\dot{q}}_{ns} = \left[\mathbf{\dot{q}}_{nr}, \  \mathbf{\dot{q}}_{r} \right ] 
= \left[(-\mathbf{J}_{nr}^{-1} \mathbf{J}_r), \    \mathbf{I} \right ]^{T}  \nabla m'_u(\mathbf{q}).
\label{eq:ns_velocity_RG}
\end{equation}


\textcolor{blue}{It is clear to see that if $\mathbf{J}_{nr}$ does not lose rank, its inversion is much more efficient than the computation of the $\mathrm{8x8}$ pseudoinverse in the PG method. Implementing both methods, PG and RG, in a MATLAB script shows that: Using the Moore-Penrose projector, the PG  requires approximately $2*10^{-3}$ s, while RG  needs only $2.5*10^{-4}$ s. The PG with the mass weighted projector needs approximately $3*10^{-3}$ s. The speed of convergence is hard to measure because the number of iterations to reach a minimum is different in both methods.
As mentioned in \cite{reduced_gradient}, PG and RG converge in different directions as soon as the redundancy degree is more than one. In Fig. \ref{fig:diff_GD_v2} one can see how the three approaches lead to different minima.  In \cite{Nemec_2}, the mass weighted projector for PG, lead to good minimization when implementing it for obstacle avoidance.  
However in this case, the mass weighted projector tends to minimize mainly w.r.t. the elbow. This might be due to the prerotation of the gradient before projecting it in the null space (see (\ref{eq:RG_dq_nemec})). Although the dimensions of both redundant joints are different, it is in general preferred to have small displacements in the linear axis than big ones in the elbow. In the Fig. \ref{fig:diff_GD_v2}, RG and PG with the Moore-Penrose pseudoinverse reach a minimum moving the linear axis approximately 0.2 m, while the PG with the mass weighted pseudoinverse needs to move approximately 60 degrees the fourth joint to achieve a minimum. To fully exploit both degrees of freedom of the robot, the PG approach with the mass weighted pseudoinverse is discarded. 
 For the tested scenarios, there are not significant differences in the rate of convergence between RG and PG with the Moore-Penrose pseudoinverse, since they reach minima not far from each other.  \\ 
Being RG analytically simpler and numerically faster, this is the method chosen to continue in this thesis. }


\begin{figure}[!htb]
	\centerline{
		\includegraphics[width=1.7\textwidth]{images/diff_GD_v2.eps}}
	\caption{Comparison of the trajectories using PG, with Moore-Penrose and mass weighted pseudoinverse, and RG.}
	\label{fig:diff_GD_v2}
\end{figure}









%
%\subsubsection{Selection of step size}
%\label{subsubsec:step_size}

%There are several affects to take into account when choosing the step size. Some of them are particular for this work and that is why a separate section is written for it.



%Between large and small step size the  differences are clear. A small step size is more likely to converge but requires more iterations. While a larger step size needs of less iterations but it is more prone to overshooting and zig-zag.

%The upper limit of the step size depends mainly on the null space motion and the oscillations. It has to be small enough to ensure that all integrated joint configurations are inside the null space grid. At the same time a high value will cause zig-zag close to the minimum. 

%As for the lower limit it will have to be high enough so the controller lead to good dynamics in the robot. 





 %\cite{chong2013introduction}


 





	
	






 



\section{Reduced Gradient algorithm}
\label{sec:1D2D}


The gradient $\nabla m'_u(\mathbf{q})$  from (\ref{eq:ns_velocity_RG}) is originally  chosen in \cite{reduced_gradient} as : 

\begin{equation}
 \nabla m'_u(\mathbf{q}) = - k \left[(-\mathbf{J}_{nr}^{-1} \mathbf{J}_r)^{T}, \   \mathbf{I} \right ] \nabla m_u(\mathbf{q})
 ,
\label{eq:RG_deLuca}
\end{equation}






where $\mathrm{\nabla m_u(\mathbf{q})}$ is the original eight-dimensional gradient from  (\ref{eq:grad_refl_mass_1}), and $\mathrm{k}$ the already mentioned scaling factor. 
By looking at (\ref{eq:ns_velocity_RG}) and (\ref{eq:RG_deLuca}), one can see how the equivalent null space projector for the RG approach is $\mathrm{\left[(-\mathbf{J}_{nr}^{-1} \mathbf{J}_r), \    \mathbf{I} \right ]^{T}  \left[(-\mathbf{J}_{nr}^{-1} \mathbf{J}_r)^{T}, \   \mathbf{I} \right ]  }$. \\
It is worth mentioning that this optimization method allows to select independently the variables used for the optimization. Meaning that the selection of the redundant joints is not constrained by this approach. 
A pseudo code for the simplified algorithm is listed in Algorithm \ref{alg:2D_alg}, where $\mathrm{\nabla m'_u(\mathbf{q})}$ (in line \ref{2D_alg:line:select}) may be obtained numerically (see Section \ref{sec:numerical}) or analytically (see Section \ref{sec:analytical}). Once $\mathrm{\nabla m'_u(\mathbf{q})}$ is obtained, the velocity for the null space motion is obtained using (\ref{eq:ns_velocity_RG}). This velocity is integrated using a explicit Euler method. To improve the accuracy, a correction term is used. An inherent characteristic of GD methods are the criss-cross patterns (zigzagging). The algorithm checks then if these oscillations appear  close to the minimum. The zigzag and the strategies to deal with it are discussed in more detail in Sections \ref{subsubsec:zigzag} and  \ref{subsec:compare_local_minim_methods}.
The last step is checking for joint limit violations. The joint limit avoidance is treated in detail in Section \ref{subsec:wf_JLA}.


\begin{algorithm}[H]
	\caption{2D Minimization}
	\label{alg:2D_alg}
	%	{\fontsize{9}{9}\selectfont
	\begin{algorithmic}[1]
		\State $\mathbf{q}_1 \leftarrow \mathbf{q}$

		
		%		\State $\dot{\mathbf{q}}_0 \leftarrow -(I - J^\dagger(\mathbf{q}) J(\mathbf{q})) \nabla m_u (\mathbf{q})$
		%		
		%		\State $\dot{\mathbf{q}}_0 \leftarrow \frac{\dot{\mathbf{q}}_0}{||\dot{\mathbf{q}}_0||}$			
		
		\For{$i \leftarrow 1$ to $n_{\max}$}
		
		\State $\dot{\mathbf{q}}_{i+1} \leftarrow  \left[\mathbf{I} \ (-\mathbf{J}_{nr}^{-1}(\mathbf{q}_i) \mathbf{J}_r(\mathbf{q}_i))  \right ]  \nabla m'_u(\mathbf{q}) $ \label{2D_alg:line:select}
		
		%		\State $\dot{\mathbf{q}}_{i+1} \leftarrow \mathrm{sign}(\dot{\mathbf{q}}_{i+1}^T \dot{\mathbf{q}}_{i}) \dot{\mathbf{q}}_{i+1}$
		
		\State $\mathbf{q}_{i+1} \leftarrow \mathbf{q}_i + \Delta t \, \dot{\mathbf{q}}_{i+1}$
		
		%		\If{$ zigzag$}
		%		\State \textbf{increase $n_{\max}$} \label{2D_alg:line:zigzag}
		%		\EndIf
		\State $\mathrm{zigzag \ strategy}$ \label{2D_alg:line:zigzag}
		
		%		\State $m_u(\mathbf{q}_{i+1}) \leftarrow [\\mathbf{u}^T \Lambda_{v}^{-1}(\mathbf{q}_{i+1}) \\mathbf{u}]^{-1}$
		
		\If{$|q_{i}| > |q_{limit}|$}
		
		\State $\mathrm{Joint\ Limit\ Avoidance \ }$ \label{2D_alg:line:JLA}
		
	

		\EndIf
		
		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i+1}$ 
		
		
		
		
		%%THE NEXT IF FROM NICO'S CODE IS NOT USED BECAUSE I GUARANTEE THE MASS IS MINIMIZED WHEN I OBTAIN THE VELOCITIES OF THE REDUNDANT JOINTS, NUMERICALLY OR ANALYTICALLY
		%		\If{$m_u(\mathbf{q}_{i+1}) < m_{u,\min}$}
		%		
		%		\State $m_{u,\min} \leftarrow m_u(\mathbf{q}_{i+1})$
		%		
		%		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i+1}$
		%		
		%		\State $\dot{\mathbf{q}}_{i} \leftarrow \dot{\mathbf{q}}_{i+1}$
		%		
		%		\Else
		%		
		%		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i}$		
		%		
		%		\State \textbf{break}
		%		\EndIf
		\EndFor
	\end{algorithmic}
	%	}
\end{algorithm}







Since the minimization is local one must be careful when integrating the velocity. Due to this the total joint velocity from  line \ref{2D_alg:line:select} is normalized for now. Further care must be taken with the step size since this is scaled by the gain $\mathrm{k}$. High step size may cause oscillations and low may cause slow convergence.  In practice the step size is chosen to be constant if the gradient of the cost function is Lipschitz. This is the case here because the rate of change of the reflected mass is bounded. Therefore a fixed step size is chosen.

The algorithm runs $\mathrm{n_{max}}$ iterations before sending a commanded goal position. 
The higher the number of iterations the more likely is to send a local minimum as goal position. In all tested cases (see Section   \ref{subsec:compare_local_minim_methods})  the local minimum was found in less that 100 iterations. So practically setting the number of iterations to 100 ensures that the goal position will be a local minimum. The higher $\mathrm{n_{max}}$, and the further from the minimum, the higher will be the commanded torque, and the faster it will converge to the local minimum.
However, a high number of iterations has two disadvantages. First of all, in each iteration some matrices have to be computed and inverted. This causes low computational efficiency. This may be solved by more efficient ways to compute these matrices and their inverses. However, the main problem of a high number of iterations is that the robot may go through local maxima when going to the local minimum This is explained in Fig.\ref{fig:bad_case_3d} and Fig.\ref{fig:bad_case_contour}.\\
%
\begin{figure}[H]
	\centerline{
		\includegraphics[width=0.9\textwidth]{images/bad_case_3d.eps}}
	\caption{Comparison between the trajectory described by the robot for low number of iterations (red) and for high number of iterations (black). The color-bar indicates the different masses.}
	\label{fig:bad_case_3d}
\end{figure}
\begin{figure}[H]
	\centerline{
		\includegraphics[width=1.4\textwidth]{images/bad_case_contour_ubuntu.eps}}
	\caption{Isocontour lines for the comparison between the trajectory described by the robot for low number of iterations (red) and for high number of iterations (black). The color-bar indicates the different masses.}
	\label{fig:bad_case_contour}
\end{figure}
%
Starting on a maximum, the robot  follows the red trajectory for low number of iterations. For high number of iterations the black trajectory is followed. If the number is high enough to find a local minimum before the first position is commanded, the robot may go trough configurations which higher reflected mass (represented by the local maxima in the middle of the figure). These intermediate masses could be higher than the original one, which makes the robot unsafer. Therefore a low number of iterations is necessary.





















%\subsection{2D minimization}
\label{subsec:2Dminim}




\subsection{Analytical}
\label{sec:analytical}

The change of the reflected mass $\mathrm{{\dot{m}_u}}$ can be expanded as

\begin{equation}
{\dot{m}_u}=\nabla m_u(\mathbf{q}) \dot{\mathbf{q}} = \left[ \frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q}_{r}}} + \frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q}_{nr}}}  (- \mathbf{J}_{nr}^{-1} \mathbf{J}_r) \right ]  \mathbf{\dot{q}}_r ,
\label{eq:mass_expanded}
\end{equation}

where the null space constraint (\ref{eq:ns_constraint}) has been used to separate between redundant and non-redundant joints. By defining

\begin{equation}
\mathbf{J}_{ma} = \left[ \frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q}_{r}}} + \frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q}_{nr}}}  (- \mathbf{J}_{nr}^{-1} \mathbf{J}_r) \right ] ,
\label{eq:jma}
\end{equation}

we have 
\begin{equation}
\mathbf{\dot{q}}_r = \mathbf{J}_{ma}^{-1} {\dot{m}_u},
\label{eq:qr_jma}
\end{equation}

where ${\mathrm{\dot{m}_u}}$ can be forced  to be negative by formulating it as the difference between a desired reflected mass $\mathrm{m_{des}}$ and the reflected mass in the current configuration $\mathrm{m_{u_{current}}}$, scaled by a gain $\mathrm{\gamma}$


\begin{equation}
{\dot{m}_u} =  \gamma (m_{des} - m_{u_{current}}).
\label{eq:m_dot_analytical_1}
\end{equation}

By again imposing the null space constraint, one obtains all joint velocities to minimize the mass

\begin{equation}
\mathbf{\dot{q}} = [\mathbf{I} \quad  -\mathbf{J}_{nr}^{-1} \mathbf{J}_r]^T   \mathbf{J}_{ma}^{-1} \gamma (m_{des} - m_{u_{current}})  .
\label{eq:qdot_analytical}
\end{equation}

As expected this result has the same form as (\ref{eq:ns_velocity_RG}), where instead of the inverse we work with the transpose of the Jacobian. And the scaling factor $\mathrm{k}$ in (\ref{eq:ns_velocity_RG}) is the factor $\mathrm{\dot{m}_u}$ from (\ref{eq:m_dot_analytical_1}). The fact that the scaling factor is proportional to the current reflected mass of the robot improves the dynamics of the robot, increasing the velocity of the redundant joints when this mass is high far from the minimum, and decreasing it when is lower. 
The effect of this parameter, together with the ones introduced in the Algorithm \ref{alg:2D_alg}, is discussed in detail in Chapter \ref{ch:experiments}.



\subsection{Numerical}
\label{sec:numerical}


In this section the RG is computed numerically. By looking at (\ref{eq:RG_deLuca}) one can see that, when computing the RG  analytically, the null space constraint is imposed. This constraint has to be  imposed when working with numerical values as well. This is clear to see if one  tries to  implement the reduced gradient straightforward, i.e. with the original gradient w.r.t. the redundant joints.  The quantities in (\ref{eq:grad_refl_mass_2}) can be expanded as follows
\begin{equation}
\frac{\partial {\mathbf{\Lambda}_{v}^{-1}}}{\partial {q_i}} = \frac{\mathbf{\partial {J}}_v}{\partial {q_i}}\mathbf{M}^{-1} \mathbf{J}_v^T + \mathbf{J}_v \frac{\mathbf{\partial {M^{-1}}}}{\partial {q_i}} {\mathbf{J}_v}^T + \mathbf{J}_v \mathbf{M}^{-1} \left ( \frac{\partial \mathbf{{J}_v}}{\partial {q_i}} \right )^T  ,
\end{equation}
\begin{align}
\frac{\partial {(\mathbf{u^T} \mathbf{\Lambda}_{v}^{-1} \mathbf{u})}}{\partial {\mathbf{u}}}& = 2 \mathbf{\Lambda}_{v}^{-1} \mathbf{u} \\
%	\partialfrac{\mathbf{u}}{q_i} & = \partialfrac{^0R_{EE}}{q_i} \mathbf{u}_{EE} \\
\frac{\partial {\mathrm{\mathbf{M}}^{-1}}}{\partial {q_i}} & = -\mathbf{M}^{-1} 
\frac{\partial {\mathrm{\mathbf{M}}}}{\partial {q_i}} \mathrm{\mathbf{M}}^{-1},
\end{align}


where $\mathrm{\mathbf{J_v}}$ comes from the equation (\ref{eq:jacobian_expanded}). In the targeted robotic system the first joint, $\mathrm{q_1}$, is of type translational. Therefore neither  the Jacobian or the mass matrix depend on $\mathrm{q_1}$. Causing the derivative of the reflected mass w.r.t. this joint to be zero:


\begin{equation}
\frac{\partial {{\mathbf{J}_v}}}{q_1} = \frac{\partial {{\mathbf{M}^{-1}}}}{q_1} = 0,
\label{eq:j_m_no__q1}
\end{equation}


so 
\begin{equation}
\frac{\partial {{\mathbf{\Lambda}_{v}^{-1}}}}{q_1} = 0.
\end{equation}

And like $\mathbf{u}$ does not depend on the joint configuration, the partial derivative of the reflected mass w.r.t. $\mathrm{q_1}$ is zero as well.
As already explained, this is so because the null space constraint is not included in the formulation of the reflected mass (see  (\ref{eq:reflected_robot_mass})). Therefore we have  unconstrained optimization.  
To impose the constraint in the differentiation of the reflected mass one can work with full derivatives. But to differentiate $\mathrm{q_1}$ w.r.t. the other joints, the inverse kinematics of the robotic system are needed. For the 7-DOF LWR the analytical inverse kinematics can be computed, like done in \cite{analyticalInverseKinematicComputation}. However, computing the inverse kinematics for the 8-DOF here considered is not straightforward, and it is not treated in this thesis.


%\textcolor{red}{\st{In the paper \textit{Kinematic Modeling and Redundancy Resolution for Nonholonomic Mobile Manipulators} de Luca  mentions nonholonomic constraints for the platform cz it cannot move in any direction.} } \correction{I do not have time for this crap}

% For a more detailed explanation on robots with translational joints refer to \cite{cartesianrobot}.





%\textcolor{magenta}{ \st{By computing the gradient in this way a clear advantage becomes visible. It is not possible to find false minima as it can be a saddle point. This is a problem that appeared in Section GBM  projecting directly the gradient of the mass into the null space.} } \correction{this saddle point is solved actually in general with the attractive potential approach}


The gradient is then computed performing small displacements inside the null space grid. Starting in an initial joint configuration, first the gradient w.r.t. the elbow is computed. For this a velocity like (\ref{eq:smms_velocity}) is necessary. It is interesting to notice that this first step is like considering the LWR alone, without the linear axis since its positions is constant. The approximation is done dividing the difference between both  masses by the difference between both positions of the elbow. 



\begin{equation}
\frac{\partial {m_u(\mathbf{q})}}{\partial {q_4}} \approx \frac{m_{new}(\mathbf{q}_{new}) - m_{actual}(\mathbf{q})}{ q_{4_{new}} - q_{4_{actual}}}.
\label{eq:approx_grad_q4}
\end{equation}



Because the mass depends on all joint positions, it is necessary to obtain them every time, to evaluate the gradient. The need of computing the mass in each iteration, makes this approach more computationally expensive than the analytical approach. To obtain the gradient w.r.t. the linear axis, $\mathrm{q_1}$, an analog process is performed. For a motion inside a self-motion manifold with constant $\mathrm{q_4}$, one can use as select-function an equation analog to (\ref{eq:smms_velocity}) but forcing the fourth entry to be zero instead of the first one. 
Once we obtain the gradient (or a close approximation) w.r.t. both $\mathrm{q_1}$ and $\mathrm{q_4}$, the velocities of the six non redundant joints are obtained. 
Another option would be determine the new positions of $\mathrm{q_1}$ and $\mathrm{q_4}$ after a small displacement on the direction of their gradients and via inverse kinematics obtain the remaining joint coordinates. But, as already mentioned, obtaining the inverse kinematics of the 8-DOF robotic system is out of the scope of this thesis.



\section{Decoupling of redundant joints}
\label{sec:Decoupling}
%\subsubsection{Weighting}
\label{subsubsec:weighting}

%\subsection{1D minimization}
%\label{subsec:1Dminim}

With the RG approach we are computing the gradient w.r.t. two joints and directly using it as the velocity of the redundant joints. The velocities of these two joints ($\mathrm{q_1}$ and $\mathrm{q_4}$) are proportional to these gradients. This allows us to weight the relation between both gradients, making it possible to reach a local minimum
by moving for example only the linear axis or only the elbow. Since the units of both joints are different (the linear axis displacement is measured in meters and the elbow in radians) the relation between them may be weighted freely.
The relation chosen to weight both joints is a linear one. So the new joint velocities are: 

\begin{equation}
	{\dot{q}}_{1_{new}}  = {\dot{q}}_{1}WF,
	\label{eq:decoupling_q1} 
\end{equation} 
\begin{equation}
	{\dot{q}}_{4_{new}} = {\dot{q}}_{4}(1-WF),
	\label{eq:decoupling_q4}
\end{equation}


where $\mathrm{WF \in [0,1]}$, is the weighing factor variable.
When $\mathrm{WF=1}$ the minimization is done only w.r.t. the linear axis , and when $\mathrm{WF=0}$ only by the elbow (see Fig.\ref{fig:wf}) will move.
For the  remaining experiments no weighting is considered, which can be regarded as $\mathrm{WF=1}$.

\begin{figure}[!h]
	\centering	
	\subfigure[]{\label{fig:wf_1}}{\includegraphics[height=0.5\textwidth]{images/wf1.eps}} 	\subfigure[]{\label{fig:wf_0}}{\includegraphics[height=0.5\textwidth]{images/wf0.eps}} 	 	
	\caption{Fig.\ref{fig:wf_1}:  Minimization for weighting factor of 1 (only  linear axis motion). \\ Fig.\ref{fig:wf_0}: Minimization for weighting factor of 0 (only elbow motion). }
	\label{fig:wf}
\end{figure}

In the numerical approach $\mathrm{{\dot{q}}_{1}}$ and $\mathrm{{\dot{q}}_{4}}$ correspond to the negative approximated gradient from (\ref{eq:qr_jma}). In the analytical approach the RG is obtained in (\ref{eq:RG_deLuca}) imposing the null space constraint. Therefore, the decoupling formulas from (\ref{eq:decoupling_q1}) and (\ref{eq:decoupling_q4}), may be applied  for $\nabla m'_u(\mathbf{q}(1))$ and  $\nabla m'_u(\mathbf{q}(2))$ respectively.



\subsection{Analysis of the Self-Motion}





Previously the velocity has been normalized before integrating. Applying the GD, it is likely that the minimum is first reached in one dimension, then in the other one. An example of this is depicted in Fig. \ref{fig:weird case}. In  Fig. \ref{fig:gradients_weird_case} one can see how the gradients w.r.t. the linear axis %($\frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q_{1}}}}$) 
and the elbow %($\frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q_{4}}}}$) 
decrease within time. At 114.45 s a minimum along the linear axis, but the gradient w.r.t. the elbow is still not zero. When this occurs there is an abrupt change in the joint velocities, see Fig.\ref{fig:velocities_weird_case}.
This is because when the gradient w.r.t. the linear axis %$\frac{\partial {m_u(\mathbf{q})}}{\partial{\mathbf{q_{1}}}} = 0$ 
reaches zero, the minimization is done inside a self-motion manifold slice for constant position of the linear axis.

\begin{figure}[!htb]
	\centering	
	\subfigure[]{\label{fig:gradients_weird_case}}{\includegraphics[height=0.46\textwidth]{images/gradients_weird_case.eps}} 	\subfigure[]{\label{fig:velocities_weird_case}}{\includegraphics[height=0.46\textwidth]{images/velocities_weird_case.eps}} 	 	
	\caption{Fig.\ref{fig:gradients_weird_case}: Gradient of the reflected mass w.r.t. both redundant joints during the minimization. \\ Fig.\ref{fig:velocities_weird_case}: Normalized joint velocities before and after the minimum in a self-motion manifold is found. }
	\label{fig:weird case}
\end{figure} 






For a better understanding, a different way of obtaining the velocity of the non redundant joints is used here. Clearly this new way yields the same results as using  (\ref{eq:ns_constraint}). After all it is a system with two degrees of freedom and by imposing the six constraints of the null space motion only one $\mathbf{\dot{q}}$ can result.
The joint velocities, $\dot{\mathbf{q}}$, are now determined as a linear combination of the unitary vectors that span the null space.
The Jacobian matrix $\mathbf{J}$ can be decomposed via singular value decomposition \cite{svd} as follows
\begin{equation}
\mathbf{J}(\mathbf{q})=\mathbf{U}(\mathbf{q}) \mathbf{S}(\mathbf{q}) \mathbf{V}(\mathbf{q})
\label{eq:svd}
\end{equation}

where the null space can be geometrically interpreted when considering 
\begin{equation}
\mathbf{V}(\mathbf{q})=(\mathbf{X}(\mathbf{q}),\mathbf{Y}(\mathbf{q})) ,
\label{eq:4.32Dietrich}
\end{equation}

where the rows of $\mathbf{Y}(\mathbf{q})$ span the null space. In our case with a 8-DOF robot, $\mathbf{Y}(\mathbf{q})$  is composed of the last two rows of $\mathbf{V}(\mathbf{q}) = [ \mathbf{v_1}, \dots , \mathbf{v_8}]$, namely $\mathbf{v_7}$ and $\mathbf{v_8}$. And like the null space has the properties of a vector space any linear combination of this two rows will result in a null space motion. 

\begin{equation}
\dot{q_1}  = C_1 v_7(1) + C_2 v_8(1) 
\label{eq:}
\end{equation}

\begin{equation}
\dot{q_4} = C_1 v_7(4) + C_2 v_8(4)
\label{eq:}
\end{equation}



By solving this system of equations we calculate $\mathrm{C_1}$ and $\mathbf{C_2}$ and use them to obtain the joints velocity:

\begin{equation}
\mathbf{\dot{q}} = C_1 \mathbf{v}_7 + C_2 \mathbf{v}_8 .
\label{eq:}
\end{equation}

When the first gradient reaches the minimum \begin{equation}
\dot{q_1}  = C_1 v_7(1) + C_2 v_8(1) = 0 .
\label{eq:}
\end{equation}

Now let us assume  $v_8(1)$ is zero, which is possible as these values come from the SVD. Then, after normalizing the velocity, we obtain $\mathbf{\dot{q} =v_7 }$. This causes the normalized velocities to be constant in  \ref{fig:velocities_weird_case} after 114.45 s.\\
%
A motion inside a self-motion manifold slice is equivalent to a motion of the 7-DOF LWR only. And for the 7-DOF LWR the analytical formula of the kernel of the Jacobian (\ref{eq:dq_ns}) depends only on the joint configuration. This is because in the end the normalized velocity from (\ref{eq:smms_velocity}), is the same as the normalized velocity from (\ref{eq:dq_ns}) adding a zero at the beginning. Since the LWR the kernel of the Jacobian has one dimension,  a unique normalized vector (with 7 entries) spans it. \\
The conclusion is that, when minimizing only in the LWR, setting a velocity proportional to the gradient, and later normalizing this velocity, is not strictly the GD method. Because the velocity of the third rotational joint depends only on the configuration of the robot. The descent direction is still correct but the step size will not be proportional to the change of the reflected mass w.r.t. this joint. 
This only occurs when minimizing inside a self-motion manifold slice with constant $\mathrm{q_1}$.  And it does not mean that the minimum will not be found. It means that the velocity, at which it will be found, is not related to the gradient itself.
A non-normalized velocity will be considered in  Chapter  \ref{ch:experiments}.



%%\begin{equation}
%%\ker(\vJ(\vq))  =\begin{pmatrix} -0.06084s_6 c_3 s_4^2\\
%%0. 6084s_6 s_4^2 s_3 s_2\\
%%0.00156s_4 s_6 (-40s_2)...\\
%%...-39s_2 c_4 +39c_2 c_3 s_4\\
%%0\\
%%-0.00156s_4 s_2 (-40c_4 s_6)+...\\
%%...40s_4 c_5 c_6 -39s_6\\
%%-0.06241s_6 s_4^2 s_2 s_5\\
%%0.0624s_4)^2 s_2 c_5	
%%\end{pmatrix}	.
%%\label{eq:dq_ns}
%%\end{equation}



\subsection{Decoupling Minimization for Joint Limit Avoidance}
\label{subsec:wf_JLA}

It is interesting to see how the two-dimensional minimization problem can be divided in two one-dimensional minimization problems by switching the weighting factor in (\ref{eq:decoupling_q1}) and (\ref{eq:decoupling_q4}). Starting  $\mathrm{WF  = 0}$  one may minimize the reflected mass inside the self-motion manifold slice for the current linear axis position. Once the minimum in the self-motion manifold slice is found, the weighting factor is switched to $0$, and the minimum along the linear axis is searched for. These two steps may be performed till a local minimum in both dimensions is found. 
Clearly, a minimum in both dimensions corresponds to a minimum on the grid. But not necessarily to the closest minimum, neither to the same minimum than the classic two-dimensional minimization gives. In  Fig.\ref{fig:1D} one can see how starting in the same configuration the strategy using this \textit{decoupling} (in black) and the original one without decoupling the minimization (in red), reach different local minima. In this example the minimum reached without decoupling strategy has a lower mass. But in other scenario it could be vice versa, i.e. the \textit{decoupling}  strategy reaches a lower minimum. 	Therefore, this is no criterion to compare both strategies.
%
\begin{figure}[!htb]
	\centerline{
		\includegraphics[width=0.7\textwidth]{images/1D.eps}}
	\caption{Isocontour lines for the comparison between the trajectory obtained with the minimization strategy in 2D (red) and with the minimization strategy in 1D (black). The color-bar indicates the different masses.}
	\label{fig:1D}
\end{figure} 
%
The strategy here introduced may be of interest in other scenarios. For example in presence of obstacles on the trajectory of the linear axis, the minimization could be switched and only done with the elbow.
Another application for this approach is joint limit avoidance.
A typical strategy for joint limit avoidance is the use of repulsive potentials \cite{JLA_3}, \cite{JLA_4}. These potentials usually are proportional to the squared distance of each joint to its joint limit. 

\begin{equation} 
U_i(q_i) = 
\begin{cases}
k_{i} (q_{i} - q_{i,min})^{2}, &\quad\text{for \ }  q_i\le q_{i,min}\\
\text{0}, &\quad\text{for \ } q_{i,min} < q_i < q_{i,max}\\
k_{i} (q_{i} - q_{i,max})^{2}, &\quad\text{for \ }  q_i\ge q_{i,max}	
\end{cases}
\label{eq:JLA_dietrich}
\end{equation}


where $\mathrm{q_{i,min}}$ and $\mathrm{q_{i,max}}$, are the minimum and maximum limits of the joint $\mathrm{q_i}$.



These potentials are often implemented on the null space of the robot. Since we are already performing minimization in this null space, it may be that both tasks interfere. For this is usually used a hierarchical arrangement, where the tasks are prioritized. For this case with two tasks the goal is that the secondary task performs as well as possible without disturbing the one with higher priority. One way to imply strict compliance of this hierarchy is using augmented null space projections \cite{JLA_5}. But it might be that the sum of the dimensions of the tasks, performing in the null space, is greater than the number of degrees of freedom of the robot. In this case the last prioritized tasks will conflict with each other,  performing as good as they can but without being accomplished to their full extent \cite{JLA_5}. Although in this thesis the minimization is treated as the only null space task, it may be that in future more tasks are added. 
Due to this conflict between null space tasks, it would be preferred to embed a joint limit avoidance strategy in the minimization scheme. This is the approach proposed in this thesis, and for that we make use of the mentioned \textit{decoupling}. If  a joint limit is detected, the minimization is then performed w.r.t. only one joints, for example $\mathrm{q_1}$. Once the minimum w.r.t. $\mathrm{q_1}$ is found, or another joint limit is detected, the minimization is performed w.r.t. $\mathrm{q_4}$ only. Switching between both till a local minimum is found, or the minimization is no longer possible, in which case the algorithm stops.
Besides the advantage that the minimization and the joint limit avoidance do not interfere with each other, also oscillations close the limits are avoided. These oscillations would occur when both strategies are implemented separately, and if the minimization and the repulsive potential are commanding opposite torques simultaneously. In the line \ref{2D_alg:line:JLA}, in the Algorithm \ref{alg:2D_alg}, this strategy was initially mentioned. The pseudo code in the Algorithm \ref{alg:2D_alg_JLA} expands the Algorithm \ref{alg:2D_alg}, describing the  minimization strategy including joint limit avoidance. Before  line \ref{alg:JLA:wf_1} the RG must be obtained, either analytically or numerically. In the line \ref{alg:JLA:repulsive} the potential from (\ref{eq:JLA_dietrich}) is differentiated.




\begin{algorithm}[H]
	\caption{2D Minimization with Joint Limit Avoidance}
	\label{alg:2D_alg_JLA}
	%	{\fontsize{9}{9}\selectfont
	\begin{algorithmic}[1]
		\State $\mathbf{q}_1 \leftarrow \mathbf{q} $
%		\State $m_{u,\min} \leftarrow m_u(\mathbf{q})$
		
		%		\State $\dot{\mathbf{q}}_0 \leftarrow -(I - J^\dagger(\mathbf{q}) J(\mathbf{q})) \nabla m_u (\mathbf{q})$
		%		
		%		\State $\dot{\mathbf{q}}_0 \leftarrow \frac{\dot{\mathbf{q}}_0}{||\dot{\mathbf{q}}_0||}$			
		
		\For{$i \leftarrow 1$ to $n_{\max}$}
	
		\State	$\nabla m'_u(\mathbf{q}(1)) = WF \nabla m'_u(\mathbf{q}(1))$
		\label{alg:JLA:wf_1}
		
		\State	$\nabla m'_u(\mathbf{q}(2)) = (1-WF) \nabla m'_u(\mathbf{q}(2))$
		\label{alg:JLA:wf_2}
		
		\State $\dot{\mathbf{q}}_{i+1} \leftarrow  \left[\mathbf{I} \ (-\mathbf{J}_{nr}^{-1}(\mathbf{q}_i) \mathbf{J}_r(\mathbf{q}_i))  \right ]  \nabla m'_u(\mathbf{q}) $ 
		\label{alg:JLA:projector}
		
		%		\State $\dot{\mathbf{q}}_{i+1} \leftarrow \mathrm{sign}(\dot{\mathbf{q}}_{i+1}^T \dot{\mathbf{q}}_{i}) \dot{\mathbf{q}}_{i+1}$
		
		\State $\mathbf{q}_{i+1} \leftarrow \mathbf{q}_i + \Delta t \, \dot{\mathbf{q}}_{i+1}$
		
		%		\If{$ zigzag$}
		%		\State \textbf{increase $n_{\max}$} \label{2D_alg:line:zigzag}
		%		\EndIf
		\State $\mathrm{zigzag \ strategy}$ 
		
		%		\State $m_u(\mathbf{q}_{i+1}) \leftarrow [\\mathbf{u}^T \Lambda_{v}^{-1}(\mathbf{q}_{i+1}) \\mathbf{u}]^{-1}$

		
%		\State $m_{u,\min} \leftarrow m_u(\mathbf{q}_{i+1})$ 
		

		
		
		
		
		%		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i}$		
		%		
		%		\State \textbf{break}
		\If{$|q_{i}| > |q_{limit}|$}
		
		\State $\dot{\mathbf{q}}_{i+1} \leftarrow -k \left[\mathbf{I}-\mathbf{J}^{\#}(\mathbf{q})\mathbf{J}(\mathbf{q})\right]  \frac{\partial {U(\mathbf{q})}}{\partial {\mathbf{q}}}	$ \label{alg:JLA:repulsive}
		
		\State $\mathbf{q}_{i+1} \leftarrow \mathbf{q}_i + \Delta t \, \dot{\mathbf{q}}_{i+1}$
		
		
		

		
		\Else
		\If{$|q_{i+1}| > |q_{limit}|$}
		
		\State $\mathrm{WF \sim= WF }$ 
		\State $\mathbf{q}_{i+1} \leftarrow \mathbf{q}_i$
		
		\EndIf
		
		
		
		
		\EndIf
		
		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i+1}$ 
		
		%%THE NEXT IF FROM NICO'S CODE IS NOT USED BECAUSE I GUARANTEE THE MASS IS MINIMIZED WHEN I OBTAIN THE VELOCITIES OF THE REDUNDANT JOINTS, NUMERICALLY OR ANALYTICALLY
		%		\If{$m_u(\mathbf{q}_{i+1}) < m_{u,\min}$}
		%		
		%		\State $m_{u,\min} \leftarrow m_u(\mathbf{q}_{i+1})$
		%		
		%		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i+1}$
		%		
		%		\State $\dot{\mathbf{q}}_{i} \leftarrow \dot{\mathbf{q}}_{i+1}$
		%		
		%		\Else
		%		
		%		\State $\mathbf{q}_{m_u}^\ast \leftarrow \mathbf{q}_{i}$		
		%		
		%		\State \textbf{break}
		%		\EndIf
		\EndFor
	\end{algorithmic}
	%	}
\end{algorithm}



  In the Fig.\ref{fig:wf_for_jla} this approach is depicted. The non-colored part of the grid represents the area beyond the joint limits. The robot starts in a configuration beyond the limits, so first the repulsive potential takes it to the reachable area. Once there, the minimization is done without decoupling. When the limits are reached again,  the minimization is done w.r.t. the elbow only, till the local minimum is found. In Fig.\ref{fig:wf_for_jla} one can see how there is a  small displacement of the linear axis when it reaches the non-reachable area. Due to its inertia, the linear axis trespass the joints limits, but the potential pushes it quickly to the reachable area.



\begin{figure}[!htb]
	\centerline{
		\includegraphics[width=0.8\textwidth]{images/JLA_weighting_v2.eps}}
	\caption{Joint limit avoidance using decoupling. First the robot if pushed to the reachable area (red trajectory) and then the reflected mass is minimized avoiding joint limits (black trajectory).}
	\label{fig:wf_for_jla}
\end{figure}




%\textcolor{red}{\textbf{THE NEXT PARAGRAPH CAN BE COMMENTED}
%\color{red} this method has to be simpler explained or maybe even omitted because it is not really gradient descent method} For the LWR an implementation of this method was used in Nico’s paper {\color{red} hacer referencia y contar mas sobre su paper y su aproach} where the movement inside the null space was based on the kernel of the Jacobian found by Christian Ott {\color{red} hacer referencia}. This worked for the case of 1D null space so one intuitive way to solve the iterative minimization for the 2D null space case was find the next local minimum inside the initial self-motion-manifold-slice, and then “jump” between manifolds.  Instead of calculating the grid in 4 parts as it was done in previous work on this project, here it is calculated in 2 halves so the gradient descent method can be applied to the whole self-motion-manifold-slice. Then it jumps to the next self-motion-manifold-slice starting the gradient descent method in an equivalent point to the minimum in the previous self-motion-manifold-slice. And jumps between self-motion-manifold-slices till it finds the local minimum respect to both dimensions (not only inside self-motion-manifold-slices, i.e. with q1 constant, but also in the integration along q4). {\color{red} Here a more detailed explanation of the process is given: Once inside one self-motion-manifold-slice it gets the dq (either via Ott's kernel and because we know that inside one self-motion-manifold-slice dq1 is 0, or using Fabian's mehod based on SVD using all joints' positions because we know them). Then it applies the 1D gradient descent method. And then we jump to another self-motion-manifold-slice: if we used Fabian's method then we got $v_7$ and $v_8$ so we just have to orthogonalize to get the new dq; and if we used Ott's kernel then we need to do what we did not before: use Fabian's method based on SVD using all joints' positions because we know them to get $v_7$ and $v_8$ so we can orthogonalize to get the new dq. When jumping between self-motion-manifold-slices it is in both cases the approach that Fabian used but Ott's kernel is only used when working inside one self-motion-manifold-slice. INSTEAD OF CALLING IT FABIAN'S APPROACH EXPLAIN IT BETTER AS GRAMM-SCHMITT ORTHONORMALIZATION} .............................................................................32233020002222222222222222222222222222}



\section{Other Line Search Techniques}
\label{sec:othermethods}



\label{subsubsec:zigzag}


A problem that comes up when using GD is the zigzagging: In a curved flat valley the optimization algorithm may zigzag slowly towards the minimum. 
%These oscillations are observed testing the algorithm in the grid. Without yet being implemented in the controller.\textcolor{blue}{why is this crossed?? }
To solve this, the first attempt is to decrease the fixed step size. But in order to get rid of the zigzag it has to be too small and the motion is too slow.
Secondly, other minimization strategies are considered. Methods like the Conjugate Gradient Descent (CGD) add a friction term in each iteration. Another way to compensate for the slow convergence of the the GD is to use Newton or Quasi-Newton's methods. These minimization strategies are reviewed in this section.



\subsection{Conjugated Gradient Descent}
\label{subsec:CJD}



While in the gradiet descent method the descent direction was chosen as 

\begin{equation}
d_k = -g_k ,
\end{equation} 

for each iteration $k$, in the CGD method the direction is

\begin{equation}
d_k = -g_k + \beta_k d_{k-1},
\end{equation}

where $g_k$ is the gradient at the $kth$ iteration. These methods add a frictional term $\beta_k$ to avoid sharp turns when getting into a (ill-conditioned) narrow valley. Since each step depends on the two last values of the gradient the zigzagging is reduced. 
%
In the literature there are many variations of the CGD. In this work we will implement  the methods from Polak-Ribere \cite{polak},  Hestenes-Stiefel \cite{hestenes} and Dai-Yuan \cite{dai}. Each of these authors propose a different formulation for $\beta_k$. Which has to be chosen so the directions of $d_{k}$ and $d_{k-1}$ are conjugate w.r.t. the Hessian of the object function.


\begin{itemize}
	\item Polak-Ribiere
	
	\begin{equation}
	\beta_k = \frac{ g_k^{T} (g_k - g_{k-1})}{ g_{k-1}^{T} g_{k-1}}	.
	\end{equation}
	
	\item Hestenes-Stiefel
	
	\begin{equation}
	\beta_k = - \frac{ g_k^{T} (g_k - g_{k-1})}{d_{k-1} (g_k - g_{k-1})}	.
	\end{equation}	
	
	\item Dai-Yuan
	
	\begin{equation}
	\beta_k =- \frac{ g_k^{T} g_k }{d_{k-1} (g_k - g_{k-1})}		.
	\end{equation}
	
	
\end{itemize}





%A problem encountered when implementing these formulas is that if the step size is small the narrow valley can be too narrow. 

A problem encountered when implementing these approaches is that these frictional terms may have more weight than the gradient. This term becomes too large when it approaches the minima, causing the optimization not to converge. In general  CGD are preferred over  GD. However, the less similar the objective function is to a quadratic function, the faster the search directions lose conjugacy, and in the worst case the CGD methods will not even converge \cite{trond}. A better approach could be the use of direct methods, like Golden Search method, as suggested in \cite{CGD_converge} but a review of these methods is out of the scope of this thesis.











\subsection{Newton's Methods}
\label{subsec:newton}



The main idea of these methods is to find a second order approximation (via Taylor series) to find the minimum of a function $\mathbf{f(x)}$.


\begin{equation}
{f(\mathbf{x}_k + \Delta \mathbf{x}) }\approx {f(\mathbf{x}_k) } + {\nabla f(\mathbf{x}_k)^{T} }{\Delta \mathbf{x}} + \frac{1}{2} {\Delta \mathbf{x}^{T} }\mathbf{H}_k {\Delta \mathbf{x}},
\end{equation}

where $\mathbf{H_k}$ is the Hessian matrix.  Compared to the previous methods, the Newton's methods (NM) tend to converge in fewer iterations. Although each iteration usually requires more computation, than in GD or CGD, since the second derivatives of the cost function (Hessian) are obtained in each step. \\
 %
A requirement for these methods is then a twice differentiable function.  
In the numerical approach we have only  an approximation of the first order derivative, so  we can only compute an approximation of the inverse of the Hessian. Therefore, the Quasi-Newton's methods (QNM) result more interesting for this approach. Since the NM may be implemented for the analytical approach only, the QNM are implemented in this thesis for both approaches.


\subsection{Quasi-Newton's Methods}
\label{subsec:quasinewton}


 
These methods are based on the approximation of the inverse of the Hessian:

\begin{equation}
\mathbf{B}_k \approx {\mathbf{H}_k}^{-1}. 
\end{equation}

The gradient is updated such that

\begin{equation}
\nabla f(\mathbf{x}_k + \Delta \mathbf{x})= \nabla f(\mathbf{x}_k) + \mathbf{B}_k {\Delta \mathbf{x} }
\end{equation}



The QNM do not require a computation of the Hessian in each iteration, but this one is updated instead.
For this approximation two formulas are considered, namely: Davidon–
Fletcher–Powell (DFP) \cite{DFP}, and a later update of this formula named Broyden– Fletcher– Goldfarb– Shanno (BFGS)\cite{BFGS}:

\begin{itemize}
	\item DFP
	
	\begin{equation}
	 \mathbf{B}_{k+1}^{DFP} =  \mathbf{B}_k + 	\frac{{\Delta \mathbf{x}}   {\Delta \mathbf{x}}^{T}}		
	 {{\Delta \mathbf{g}}^{T}  {\Delta \mathbf{x}}  } -  \frac{(\mathbf{B}_{k}^{T}  {\Delta \mathbf{g}}) (\mathbf{B}_{k}^{T}  {\Delta \mathbf{g}})^{T}  }
	 {{\Delta \mathbf{g}}^{T} (\mathbf{B}_{k}^{T}  {\Delta \mathbf{g}})   },
	 \label{eq:DFP}
	\end{equation}
	
	where ${\Delta \mathbf{g}} = \mathbf{g}_{k+1} - \mathbf{g}_k $.
	
	\item BFGS
	
	\begin{equation}
	\mathbf{B}_{k+1}^{BFGS} =  \mathbf{B}_{k+1}^{DFP} + {\Delta \mathbf{g}}^{T}(\mathbf{B}_{k}^{T}  {\Delta \mathbf{g}})(\mathbf{u}  \mathbf{u}^{T})	,
	\end{equation}	
	
	where $\mathbf{u} = \frac{{\Delta \mathbf{x}}}		
	{{\Delta \mathbf{g}}^{T}  {\Delta \mathbf{x}}} - \frac{(\mathbf{B_{k}}^{T}  {\Delta \mathbf{g}})   }
	{{\Delta \mathbf{g}}^{T} (\mathbf{B}_{k}^{T}  {\Delta \mathbf{g}})   }$.
	

\end{itemize}


\subsubsection{Loss of Positive Definiteness}
\label{subsubsec:loss_pdness}

When implementing these formulas, a problem that occurs is the loss of positive definiteness of the Hessian matrix. Numerical calculations, inexact line search and round-off and truncation errors may cause loss of positive definiteness. To ensure stable and convergent behavior, some safeguards must be taken. 
Using QN methods for positive definite quadratic functions, the convergence is guaranteed to an exact optimum in at most $n$ iterations, where $n$ is the number of variables (two in our case).
However, with general cost functions this is not the case and the methods need to be restarted   \cite{intro_opt_design}. \\
%
In our case, the grid (cost function) is generally not a quadratic function therefore round-off and truncation errors may occur. 
Regarding line search: A key property of the BFGS and DFP updates is that the approximated inverses of the Hessian are positive definite. This will only happen if the step size is chosen to satisfy the Wolfe conditions for each $k$ iteration:


\begin{itemize}
	\item Armijo rule
	
	\begin{equation}
	f(x_k +  \alpha_k p_k) \le f(x_k) + c_1 \alpha_k \nabla f_k^{T} p_k ,
 	\end{equation}
	
	where $\alpha_k $ is the step size, $p_k$ is the descent direction, and $c_1$ is a constant to be chosen.
	
	\item Curvature condition
	
	\begin{equation}
	\nabla f(x_k + \alpha_k p_k)^{T} \ge  c_2 \nabla f_k^{T} p_k  ,
	\end{equation}	
	
	where $c_2$ is another constant and both constants have to be chosen such that $ 0 < c_1 < c_2 < 1 $.
	
	
\end{itemize}





Since we are normalizing the velocity, the step size includes a division by the norm of the joint velocities. This step size can be escalated but it is going to be variable because the velocity, and its norm, change in each iteration. 
The Wolfe conditions are tested with the most common values of $c_1$ and $c_2$ proposed by Nocedal \cite{Nocedal2006NO}.
As expected, during the tests
the variable step size ends up violating these conditions. This causes inexact line search and according to \cite{intro_opt_design} the positive definiteness is lost.% This loss comes from an inexact line search, because the search direction is guaranteed to be that of descent for the cost function only if the approximated of the Hessian is positive definite.

\section{Discussion}
\label{sec:comparison_local_minim}


There are several ways to compute a null space velocity that minimizes the reflected mass. The ones considered in this thesis are PG and RG. The RG is chosen for being analytically simpler and numerically faster. The RG still has the problem of the zigzagging in narrow valleys. This causes a slow convergence. To improve this, other Line Search Techniques are implemented, which results are shown below.



% \subsection{Comparison of the Local Minimization Methods}
 \label{subsec:compare_local_minim_methods}
 
 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[!h]
	\centering
	\caption{Comparison Line Search methods}
	\label{table:comp_ls_methods} 
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline 
			&                                      & \multicolumn{2}{c|}{\textbf{GD}}                             & \multicolumn{2}{c|}{\textbf{DFP}}                            & \multicolumn{2}{c|}{\textbf{BFGS}}                           \\ \cline{3-8} 
			\multirow{-2}{*}{\textbf{Initial\ 
					configuration}} & \multirow{-2}{*}{\textbf{Direction}} & \textbf{Ending} & \textbf{Iterations}                          & \textbf{Ending} & \textbf{Iterations}                          & \textbf{Ending} & \textbf{Iterations}                          \\ \hline
			A                                                & x                                    & 3 zigzags              & 94                                  & 3 zigzags              & 81                                  & 3 zigzags              & \cellcolor[HTML]{34FF34}\textbf{77} \\ \hline
			B                                                & x                                    & 3 zigzags              & 88                                  & CJL                    & 68                                  & 2 zigzags              & \cellcolor[HTML]{34FF34}\textbf{67} \\ \hline
			C                                                & z                                    & 2 zigzags              & \cellcolor[HTML]{34FF34}\textbf{30} & 2 zigzags              & \cellcolor[HTML]{34FF34}\textbf{30} & 2 zigzags              & 32                                  \\ \hline
			D                                                & z                                    & 2 zigzags              & 30                                  & 2 zigzags              & 27                                  & minim. found           & \cellcolor[HTML]{34FF34}\textbf{23}                                  \\ \hline
			E                                                & y                                    & minim. found           & \cellcolor[HTML]{34FF34}\textbf{32} & minim. found           & 38                                  & minim. found           & 38                                  \\ \hline
			F                                                & y                                    & minim. found           & 37                                  & minim. found           & \cellcolor[HTML]{34FF34}\textbf{23} & 2 zigzags              & \cellcolor[HTML]{34FF34}\textbf{23} \\ \hline
			
			
			\multicolumn{8}{l}{\textbf{GD}: Gradient Descent}\\
			\multicolumn{8}{l}{\textbf{DFP}: Davidon–Fletcher–Powell}\\
			\multicolumn{8}{l}{\textbf{BFGS}: Broyden–Fletcher–Goldfarb–Shanno}\\
			\multicolumn{8}{l}{\textbf{Ending}: number of zigzags before sum of both gradients below threshold ($\mathrm{\epsilon=0.1}$)}\\
			\multicolumn{8}{l}{\textbf{CJL}: Conservative Joint Limits reached}\\
		\end{tabular}%
			} 
\end{table}


In Tab.\ref{table:comp_ls_methods} the results of tests for different initial configurations and different reflected directions are listed. There is a comparison of the GD and both QNM. As already mentioned CGD methods lead to worse convergence than GD. The Trust Region methods and the NM are not implementable for both approaches, analytical and numerical.
The tested cases were chosen so the initial configuration is relatively far from the minimum, because close to a minimum all methods perform the same.
The QNM, and in particular BFGS, are in general faster than GD. This is to be expected, since BFGS is an improved version of DFP. However, the difference is not significant. Theoretically the convergence should be much faster for the QNM but the need to restart them often, as explained in the Section \ref{subsubsec:loss_pdness}, causes the convergence rate to decrease. Being even for some cases the GD approach slightly faster. This may happen minimizing along the y- and z-direction since these grids show contour lines for the local minima. Therefore, these methods reach different minima. However, in the x-direction the grid does not show these countour lines, and all the methods converge to the same minimum. In this direction it is clear how the QNM converge faster than the GD.
In one case, one of the methods (DFP) reached the joint limits and it stopped. But no method is more prone to reach joint limits. For all this, and for sake of simplicity, the GD method is the one chosen to be implemented in the controller.\\
If zigzagging is present, the Algorithm \ref{alg:2D_alg} (see line \ref{2D_alg:line:zigzag}) stops when the gradient is smaller than a certain convergence parameter ($\mathrm{|\nabla m'_u(\mathbf{q})| < \epsilon=0.1 )}$.
In the tested grids, this convergence parameter ensures that the position is close the minimum. Therefore, the improvement between finding the desired local minimum and being below the convergence threshold is insignificant. So this is not a criterion to decide between the methods.
Nevertheless, this threshold has been set from the experiments here performed. None of the grids ever showed minima, in which vicinity the gradient changed abruptly. But it is not ensured that they do not exist for specific configurations. If existing it would be a problem because the oscillations could occur for a higher threshold than the one tested here. To improve this, the oscillations may also be decreased by increasing the number of iterations ($n_{max}$) in the Algorithm \ref{alg:2D_alg}. Then if zigzagging appears the algorithm will iterate more times, getting closer to the minimum, before sending a goal position. In this way the oscillations in the grid will not be present in the real robot. However, for very ill-defined problems this zigzagging may tend to infinite. Furthermore, the higher the number of iterations the more time the algorithm needs to compute a goal position, and this may cause real time violations. A more detailed review on the effect of $n_{max}$ is given in Chapter \ref{ch:experiments}. 

